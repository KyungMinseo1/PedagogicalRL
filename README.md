# ğŸ‘¨â€ğŸ« From Problem-Solving to Teaching Problem-Solving  
### *Aligning LLMs with Pedagogy using Reinforcement Learning*
**What if your LLM could teachâ€”rather than just answer?**  
This project provides a recipe to transform a standard instruction-tuned language model into a math tutor that actually teaches, using multi-turn **GRPO** in a classroom environment with a **synthetic student**.

LLMs today excel at solving problems, but good teaching is about **strategically withholding answers** and guiding learners through their own reasoning. This repo implements a scalable, annotation-free framework that aligns LLM behavior with **pedagogical principles** like Socratic questioning, helpful scaffolding, and solution withholding.

We train 7B-sized open models to come close to closed-source models like LearnLMâ€”**without any human labels**â€”and maintain performance on reasoning tasks like GSM8K and MATH500.

> ğŸ”§ For implementation details (multi-node setup, memory handling, vLLM coordination), see [technical.md](technical.md).  
> ğŸ” Explore tutorâ€“student conversations here: [Conversation Visualizer](https://pedagogical-rl.vercel.app)

ğŸ“„ **Paper**: [arxiv.org/abs/2505.15607](https://arxiv.org/abs/2505.15607)

**Dataset**: [BigMathRL-Filtered](https://huggingface.co/datasets/rd211/Big-Math-RL-Verified-Filtered) which is a subset of [Big-Math-RL-Verified](https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified)

ğŸ§  **Models**: We release two versions of our 7B tutor model:
- ğŸ¤— [eth-nlped/TutorRL-7B](https://huggingface.co/eth-nlped/TutorRL-7B) â€” standard version without internal planning ($\lambda=0.75$)  
- ğŸ¤— [eth-nlped/TutorRL-7B-think](https://huggingface.co/eth-nlped/TutorRL-7B-think) â€” enhanced with explicit `<think>` tags for interpretable planning (hard- $\lambda=1.0$)

ğŸ”“ **License**: CC-BY-4.0


---

<div align="center">
  <img src="images/diagram.png" alt="System diagram" style="max-height: 600px; height: auto; width: auto; max-width: 100%; vertical-align: middle;"/>
</div>

---


## ğŸ§  Core logic

The reinforcement learning loop is implemented in [`src/classroom.py`](src/classroom.py), which simulates a multi-turn dialog between:

- **Tutor (LLM under training)**
- **Student (frozen LLM)**
- **Judges (for leakage/helpfulness evaluation)**
- **Reward calculator (post-dialog solve rate & pedagogy)**

Each conversation follows a state machine:

```

START â†’ TEACHER_TURN â†’ STUDENT_TURN â†’ TEACHER_TURN â†’ ... â†’ JUDGE_TURN â†’ GENERATE_SOLUTION â†’ REWARD_TURN â†’ END

````

- The loop alternates between **tutor and student turns**
- **Student turns are masked** during loss calculation
- **Only the tutor model is updated** using GRPO
- **Conversations are processed in large batches**
- **Model weights are dynamically loaded/unloaded** across states to conserve memory

---

## ğŸš€ Quick start

```bash
# 1) create a clean environment
conda create -n pedagogy python=3.11
conda activate pedagogy

# 2) install core dependencies
pip install -r requirements.txt          # torch, trl, vllm, ...

# 3) add your keys
nano .env
````

---

## ğŸ§ª Environment variables

| Variable             | Purpose                             |
| -------------------- | ----------------------------------- |
| `HF_TOKEN`           | Pull/push models from ğŸ¤— Hub        |
| `WANDB_API_KEY`      | Weights & Biases logging (optional) |
| `OPENROUTER_API_KEY` | Optional: for LLMs via OpenRouter   |

---

## ğŸ“ Training

### 1 Â· Supervised Fine-Tuning (baseline)

```bash
accelerate launch \
  --config_file config/deepspeed/zero2_4GPU.yaml \
  train_sft.py --config-name 7b.yaml
```

### 2 Â· Pedagogical Reinforcement Learning (ours)

```bash
./start_rl_training.sh \
  --config_file config/deepspeed/zero3_4GPU.yaml \
  --config-name 7b.yaml
```

#### ğŸ§© Hard Pedagogy Mode

Use the `generation.ignore_rejected_judge` flag:

| Flag                          | Reward behavior                                             |
| ----------------------------- | ----------------------------------------------------------- |
| `ignore_rejected_judge=true`  | **Default**: Apply a relative penalty `âˆ’Î»` if rejected         |
| `ignore_rejected_judge=false` | **Hard**: Override total reward with fixed `âˆ’Î»` if rejected |

ğŸ› Other common CLI knobs:

| Flag                                                   | Description                                     |
| ------------------------------------------------------ | ----------------------------------------------- |
| `generation.extra_penalty_for_rejected_judges`         | Î» â€” penalty magnitude                           |
| `generation.number_judge_attempts=0`                   | Î» = 0 (no pedagogy constraint)                  |
| `generation.use_thinking=true` / `force_thinking=true` | Enable hidden `<think>` tags for tutor planning |

---

## ğŸ“ˆ Evaluation

### Pedagogical Benchmarks

Evaluate on BigMath-style multi-turn dialogs:

```bash
python eval.py --config-name Qwen2.5-7B-Instruct.yaml
```

### Reasoning Benchmarks (MMLU, GSM8K, MATH500)

Using [LightEval](https://github.com/huggingface/lighteval):

```bash
lighteval vllm \
  "model_name=Qwen/Qwen2.5-7B-Instruct,gpu_memory_utilization=0.85,max_model_length=8192,dtype=bfloat16,\
   generation_parameters={max_new_tokens:8192,temperature:0.6,top_p:0.95}" \
  "lighteval|math_500|0|0,helm|mmlu|5|0,lighteval|gsm8k|4|0" \
  --use-chat-template
```

---

## ğŸ§± Project layout

```
.
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ deepspeed/            # ZeRO configs
â”‚   â”œâ”€â”€ eval/                 # eval task configs
â”‚   â””â”€â”€ train_rl/             # RL training configs
â”œâ”€â”€ prompt_templates/         # system + judge prompts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classroom.py          # ğŸ§  Core RL loop (multi-agent dialog)
â”‚   â”œâ”€â”€ grpo/                 # GRPO reinforcement learning trainer
â”‚   â”œâ”€â”€ inference_providers/  # OpenRouter / Gemini adapters
â”‚   â””â”€â”€ vllm/                 # vLLM helpers (multi-node, memory mgmt)
â”œâ”€â”€ utils/                    # reward functions, judge filters, logging
â”œâ”€â”€ train_rl.py               # Entry point for RL training
â”œâ”€â”€ train_sft.py              # Supervised fine-tuning entry
â””â”€â”€ start_rl_training.sh      # Launcher for RL
```

---

## ğŸ“„ Citation

```
@inproceedings{dinucu-jianu-etal-2025-problem,
    title = "From Problem-Solving to Teaching Problem-Solving: Aligning {LLM}s with Pedagogy using Reinforcement Learning",
    author = "Dinucu-Jianu, David  and
      Macina, Jakub  and
      Daheim, Nico  and
      Hakimi, Ido  and
      Gurevych, Iryna  and
      Sachan, Mrinmaya",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.15/",
    doi = "10.18653/v1/2025.emnlp-main.15",
    pages = "272--292",
    ISBN = "979-8-89176-332-6"
```
